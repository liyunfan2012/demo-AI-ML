{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f772908-ee2e-4fac-a7f4-a7b905221143",
   "metadata": {},
   "source": [
    "# Demo Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711df77-9671-446c-a4ae-13232542f43c",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f60ab3-5ca0-41c1-b56e-00cedec6c6ca",
   "metadata": {},
   "source": [
    "Denote input $\\mathbf x$, hidden state (short term memory) $\\mathbf h\\in\\mathbb{R}^K$, cell state (long term memory) $\\mathbf c\\in\\mathbb{R}^K$. Define the function generating gates\n",
    "\n",
    "$$\n",
    "G(\\mathbf x, \\mathbf h;W_x, W_h,\\beta) = \\sigma(W_x\\mathbf x+W_h\\mathbf h+\\beta).\n",
    "$$\n",
    "\n",
    "Define function generating candidate for long-term memeory\n",
    "\n",
    "$$\n",
    "H(\\mathbf x, \\mathbf h;W_x, W_h,\\beta) = \\tanh(W_x\\mathbf x+W_h\\mathbf h+\\beta).\n",
    "$$\n",
    "\n",
    "With the same parameters, for each stage $t$ with $\\mathbf h_{t-1}$, $\\mathbf c_{t-1}$, we have forget gate \n",
    "\n",
    "$$\n",
    "f = G(\\mathbf x, \\mathbf h;W_{xf}, W_{hf},\\beta_f),\n",
    "$$\n",
    "\n",
    "input gate\n",
    "\n",
    "$$\n",
    "i = G(\\mathbf x, \\mathbf h;W_{xi}, W_{fi},\\beta_i),\n",
    "$$\n",
    "\n",
    "new cell state as\n",
    "\n",
    "$$\n",
    "c_{t} =  f\\mathbf c_{t-1}+ iH(\\mathbf x, \\mathbf h;W_{xx}, W_{fx},\\beta_x).\n",
    "$$\n",
    "\n",
    "The new hidden state is\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = G(\\mathbf x, \\mathbf h;W_{xo}, W_{ho},\\beta_o)\\tanh(\\mathbf{c}_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009a8d9-32a5-491b-8add-1b9423ed659b",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270c83f-0907-4aa8-abc2-9e025dd1ef56",
   "metadata": {},
   "source": [
    "### Generate Numpy Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e4e3dc-976b-4b69-807d-3703da9f6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "def gen_wave(wave_type: int, cycles: int, phase: float, seq_len: int):\n",
    "    \"\"\"Return a 1D NumPy array of length seq_len for the chosen wave.\"\"\"\n",
    "    t = np.linspace(0, 1, seq_len, dtype=np.float32)\n",
    "    f = float(cycles)  # cycles across [0,1]\n",
    "    if wave_type == 0:      # sine\n",
    "        x = np.sin(2 * np.pi * f * t + phase)\n",
    "    elif wave_type == 1:    # square\n",
    "        x = np.sign(np.sin(2 * np.pi * f * t + phase))\n",
    "    else:                   # sawtooth in [-1, 1]\n",
    "        frac = (f * t + phase / (2*np.pi)) % 1.0\n",
    "        x = 2.0 * (frac - 0.5)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "N_SAMPLES = 3000\n",
    "SEQ_LEN   = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "X_list, y_list = [], []\n",
    "for _ in range(N_SAMPLES):\n",
    "    wave_type = np.random.randint(0, 3)           # 0/1/2\n",
    "    cycles    = np.random.randint(5, 11)          # 5â€“10 periods\n",
    "    phase     = np.random.uniform(0.0, 2*np.pi)   # random phase\n",
    "    wave      = gen_wave(wave_type, cycles, phase, SEQ_LEN)\n",
    "    X_list.append(wave)                            # (T,)\n",
    "    y_list.append(wave_type)\n",
    "\n",
    "X = np.stack(X_list, axis=0).astype(np.float32)    # (N, T)\n",
    "y = np.array(y_list, dtype=np.int64)               # (N,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73c1a9-0c15-4776-8278-bb9b2718c901",
   "metadata": {},
   "source": [
    "### Generate DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed958872-44aa-4ae9-bf01-e98b58cb45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X_torch = torch.from_numpy(X).unsqueeze(-1)               # (N, T, 1)\n",
    "y_torch = torch.from_numpy(y)                             # (N,)\n",
    "\n",
    "dataset = TensorDataset(X_torch, y_torch)\n",
    "\n",
    "val_ratio = 0.2\n",
    "n_val = int(len(dataset) * val_ratio)\n",
    "n_train = len(dataset) - n_val\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9c730-a6a5-4700-a74a-1d42ad3fdf20",
   "metadata": {},
   "source": [
    "## Define RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e27d14c-6e47-495c-9ec8-512f2ee0da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, forget_bias: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # x->gates and h->gates (concat: [i | f | g | o])\n",
    "        self.x2g = nn.Linear(input_size, 4 * hidden_size, bias=True)\n",
    "        self.h2g = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "        with torch.no_grad():\n",
    "            H = hidden_size\n",
    "            self.x2g.bias[H:2*H].fill_(forget_bias)\n",
    "            self.h2g.bias[H:2*H].fill_(forget_bias)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        x_t:    (B, input_size)\n",
    "        h_prev: (B, H)\n",
    "        c_prev: (B, H)\n",
    "        returns: h_t, c_t\n",
    "        \"\"\"\n",
    "        gates = self.x2g(x_t) + self.h2g(h_prev)   # (B, 4H)\n",
    "        H = self.hidden_size\n",
    "        i = torch.sigmoid(gates[:, 0:H])           # input gate\n",
    "        f = torch.sigmoid(gates[:, H:2*H])         # forget gate\n",
    "        g = torch.tanh(   gates[:, 2*H:3*H])       # candidate\n",
    "        o = torch.sigmoid(gates[:, 3*H:4*H])       # output gate\n",
    "        c_t = f * c_prev + i * g                   # long-term memory\n",
    "        h_t = o * torch.tanh(c_t)                  # short-term / output\n",
    "        return h_t, c_t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc3cee-bf62-4643-81c5-c4e1137c220d",
   "metadata": {},
   "source": [
    "- Demo use of RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0856f5-8461-4dfe-9355-4e3c2fd4740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([100, 1])\n",
      "h_new: tensor([-0.0556, -0.1769,  0.1051, -0.2274], grad_fn=<SqueezeBackward1>)\n",
      "c_new: tensor([-0.0757, -0.4367,  0.3316, -0.3853], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))   # x_batch: (B, T, 1)\n",
    "x = x_batch[36]                               # a random sample, x: (100, 1)\n",
    "print(\"x shape:\", x.shape)                    # (seq_len, 1)\n",
    "\n",
    "cell = LSTMCell(input_size=1, hidden_size=4)\n",
    "h = torch.zeros(1, 4)   # keep a batch dim of 1\n",
    "c = torch.zeros(1, 4)\n",
    "\n",
    "t = 0\n",
    "# x[t] is (1,), make it (1,1) to match (B=1, input_size=1)\n",
    "x_t = x[t].unsqueeze(0)                       # (1, 1) to match (B=1,input_size=1)\n",
    "h_new, c_new = cell(x_t, h, c)\n",
    "\n",
    "print(\"h_new:\", h_new.squeeze(0))             # (4,)\n",
    "print(\"c_new:\", c_new.squeeze(0))             # (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f154ba4-6a00-486d-8f56-1bc11c5cce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=4, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = LSTMCell(input_size, hidden_size)   # just one layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F)\n",
    "        returns: logits (B, num_classes)\n",
    "        \"\"\"\n",
    "        B, T, F = x.shape\n",
    "        h = x.new_zeros(B, self.hidden_size)\n",
    "        c = x.new_zeros(B, self.hidden_size)\n",
    "        for t in range(T):\n",
    "            x_t = x[:, t, :]          # (B, F)\n",
    "            h, c = self.cell(x_t, h, c)\n",
    "        return self.fc(h)             # use last hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b199779-e97c-4dca-b6f0-eba0a96a6492",
   "metadata": {},
   "source": [
    "## Train RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afdaae-8217-4985-9249-dd6f1069f6ec",
   "metadata": {},
   "source": [
    "### Define training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16af263f-80fc-4a38-a01b-ac84e7775924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(input_size=1, hidden_size=4, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb, yb\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss   += loss.item() * xb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_samples += xb.size(0)\n",
    "    return total_loss/total_samples, total_correct/total_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb, yb\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss   += loss.item() * xb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_samples += xb.size(0)\n",
    "    return total_loss/total_samples, total_correct/total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684a479-fcaa-475e-852f-e5ceb2a2b5f3",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f8e62-95c9-4138-80dc-6c2f458a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "for epoch in range(1, n_epochs):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    va_loss, va_acc = evaluate(model, val_loader, criterion)\n",
    "    if (epoch+1)%5 ==0:\n",
    "        print(f\"epoch {epoch+1:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(val_loader))\n",
    "    preds = model(xb).argmax(1).cpu().numpy()[:10]\n",
    "    print(f\"true:{yb[:10].numpy()}\\n pred:{preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
