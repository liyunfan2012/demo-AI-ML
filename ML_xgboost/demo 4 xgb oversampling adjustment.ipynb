{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8fcc53-2bec-4300-a6a6-b85fb14bce8b",
   "metadata": {},
   "source": [
    "## Data Prepartion\n",
    "\n",
    "Prepare train, test data from https://github.com/dmlc/xgboost/tree/master/demo/data\n",
    "\n",
    "Over sample y=1 in train with beta=2. (odds ratio doubled after oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246d752-cbac-4632-9ad3-1d7d87b7fc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19618511-a9d1-4579-8983-d6f0da556a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq of y in train:\n",
      "y\n",
      "0.0    3373\n",
      "1.0    3140\n",
      "dtype: int64\n",
      "freq of y in sampled train:\n",
      "y\n",
      "0.0    1652\n",
      "1.0    3140\n",
      "dtype: int64\n",
      "freq of y in test:\n",
      "y\n",
      "0.0    835\n",
      "1.0    776\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "dtrain = load_svmlight_file('agaricus.txt.train')\n",
    "dtest = load_svmlight_file('agaricus.txt.test')\n",
    "\n",
    "fnames=['f'+str(i) for i in range(0,126)]  # same as feature naming by xgboost\n",
    "train = pd.DataFrame(dtrain[0].toarray(),columns=fnames).assign(y=dtrain[1])\n",
    "test = pd.DataFrame(dtest[0].toarray()).assign(y=dtest[1])\n",
    "\n",
    "# oversampling\n",
    "beta=2\n",
    "train['rnd']=np.random.uniform(0,1,train.shape[0])\n",
    "train_sampled=train[(train['rnd']<1/beta) | (train['y']==1.0)]\n",
    "print('freq of y in train:')\n",
    "print(train.groupby(['y']).size())\n",
    "print('freq of y in sampled train:')\n",
    "print(train_sampled.groupby(['y']).size())\n",
    "print('freq of y in test:')\n",
    "print(test.groupby(['y']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df0693-d031-43bc-ac0c-e9c2fd710119",
   "metadata": {},
   "source": [
    "## Method1ï¼šAdjustprobability\n",
    "\n",
    "For each output $p$, the adjusted probability is\n",
    "$$\n",
    "p^* = \\frac{p}{p+(1-p)*\\beta},\n",
    "$$\n",
    "$$\n",
    "\\frac{p}{1-p}/\\beta = \\frac{p^*}{1-p^*}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47d555-11f9-4b34-b5d1-9cf607635fc5",
   "metadata": {},
   "source": [
    "### Intercept only model\n",
    "\n",
    "Fit a treee with ony a root node to show the adustment for oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca2bd49-9f1a-4d1e-94da-2259c9d5df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: mean(p)=0.532, mean(y)=0.386\n",
      "train: mean(p)=0.539, mean(y)=0.482\n",
      "train_sample: mean(p)=0.631, mean(y)=0.655\n",
      "train_sample: mean(adjusted p)=0.527\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':1, 'learning_rate':1, 'objective':'binary:logistic','n_estimators':1}\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss',**param)\n",
    "xgb_model.fit(train_sampled.iloc[:,:-2],train_sampled.iloc[:,-2])\n",
    "\n",
    "pred = lambda df: pd.Series(xgb_model.predict_proba(df)[:,1])\n",
    "p_test = pred(test.iloc[:,:-1])\n",
    "p_train = pred(train.iloc[:,:-2])\n",
    "p_train_sampled = pred(train_sampled.iloc[:,:-2])\n",
    "\n",
    "print(f'test: mean(p)={p_test.mean():.3f}, mean(y)={test.iloc[:,-2].mean():.3f}')\n",
    "print(f'train: mean(p)={p_train.mean():.3f}, mean(y)={train.iloc[:,-2].mean():.3f}')\n",
    "print(f'train_sample: mean(p)={p_train_sampled.mean():.3f}, mean(y)={train_sampled.iloc[:,-2].mean():.3f}')\n",
    "print(f'train_sample: mean(adjusted p)={p_train_sampled.map(lambda x:x/(x+(1-x)*beta)).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e153d0c7-16c4-41de-9181-796e224a3ed3",
   "metadata": {},
   "source": [
    "### Three-tree model\n",
    "The average of adjusted p is close to the average of y but not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0b0c28-8048-470a-9322-8d19c1a63b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: mean(p)=0.448, mean(y)=0.386\n",
      "train: mean(p)=0.446, mean(y)=0.482\n",
      "train_sample: mean(p)=0.561, mean(y)=0.655\n",
      "train_sample: mean(adjusted p)=0.455\n"
     ]
    }
   ],
   "source": [
    "param = {'base_score':0.1,'max_depth':2, 'learning_rate':.3, 'objective':'binary:logistic','n_estimators':3}\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss',**param)\n",
    "xgb_model.fit(train_sampled.iloc[:,:-2],train_sampled.iloc[:,-2])\n",
    "\n",
    "pred = lambda df: pd.Series(xgb_model.predict_proba(df)[:,1])\n",
    "p_test = pred(test.iloc[:,:-1])\n",
    "p_train = pred(train.iloc[:,:-2])\n",
    "p_train_sampled = pred(train_sampled.iloc[:,:-2])\n",
    "\n",
    "print(f'test: mean(p)={p_test.mean():.3f}, mean(y)={test.iloc[:,-2].mean():.3f}')\n",
    "print(f'train: mean(p)={p_train.mean():.3f}, mean(y)={train.iloc[:,-2].mean():.3f}')\n",
    "print(f'train_sample: mean(p)={p_train_sampled.mean():.3f}, mean(y)={train_sampled.iloc[:,-2].mean():.3f}')\n",
    "print(f'train_sample: mean(adjusted p)={p_train_sampled.map(lambda x:x/(x+(1-x)*beta)).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490fd6b-49c1-4a34-8936-ce6939cac130",
   "metadata": {},
   "source": [
    "## Method 2: adjust model base_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594b26d-9f3d-46c8-84c8-9309efc29c2a",
   "metadata": {},
   "source": [
    "### Fit and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ac982e-29af-4496-b190-7b364e08957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'base_score':0.1,'max_depth':2, 'learning_rate':.5, 'objective':'binary:logistic','n_estimators':3}\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss',**param)\n",
    "xgb_model.fit(train_sampled.iloc[:,:-2],train_sampled.iloc[:,-2])\n",
    "xgb_model.save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e204cf-c171-4ac0-a35e-a9605ea2bd0d",
   "metadata": {},
   "source": [
    "### Load and mondify model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098586f9-073b-414f-9246-b322c7ba8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_model.json') as json_file:\n",
    "    model_json = json.load(json_file)\n",
    "\n",
    "learner_model_param = model_json['learner']['learner_model_param']\n",
    "base_score = float(learner_model_param['base_score'])\n",
    "learner_model_param['base_score'] = str(round(base_score/(base_score+(1-base_score)*beta),8))\n",
    "\n",
    "with open(\"xgb_model1.json\",\"w\") as outfile:\n",
    "    json.dump(model_json,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49a0fc-fb5d-44a1-8bde-7d4e2a62740b",
   "metadata": {},
   "source": [
    "### Load modified model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7049f579-f6a0-48e5-83b4-12aa4d9c5e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: aveg(adj p)=0.4668,avg(p)=0.5017\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.special import logit,expit\n",
    "xgb_model1 = xgb.XGBClassifier()\n",
    "xgb_model1.load_model(\"xgb_model1.json\")\n",
    "\n",
    "p1 = pd.Series(xgb_model1.predict_proba(train.iloc[:,:-2])[:,1])\n",
    "p = pd.Series(xgb_model.predict_proba(train.iloc[:,:-2])[:,1])\n",
    "\n",
    "print(f'Training data: aveg(adj p)={p1.mean():.4f},avg(p)={p.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6537039-946a-4369-9b90-1e79d7cbd216",
   "metadata": {},
   "source": [
    "## Method 3: Calibration by Logistic Regresion\n",
    "### Load xgb model and model json file\n",
    "\n",
    "Trees are outputs by xgb_model as\n",
    "```python\n",
    "xgb_model0.get_booster().get_dump()\n",
    "```\n",
    "Leaf outputs are in the following lists from json\n",
    "```python\n",
    "model_json['learner']['gradient_booster']['model']['trees']['split_conditions']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f91d5f-5925-46bb-ba2c-b5e29adf2038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[f28<0.5] yes=1,no=2,missing=1\n",
      "\t1:[f55<0.5] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=4.78408623\n",
      "\t\t4:leaf=0.201096877\n",
      "\t2:[f108<0.5] yes=5,no=6,missing=5\n",
      "\t\t5:leaf=-0.395252198\n",
      "\t\t6:leaf=4.13344908\n",
      "\n",
      "0:[f59<0.5] yes=1,no=2,missing=1\n",
      "\t1:[f66<0.5] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=0.131180301\n",
      "\t\t4:leaf=4.32168531\n",
      "\t2:leaf=-6.03941679\n",
      "\n",
      "0:[f28<0.5] yes=1,no=2,missing=1\n",
      "\t1:[f22<0.5] yes=3,no=4,missing=3\n",
      "\t\t3:leaf=0.435074449\n",
      "\t\t4:leaf=-1.07391596\n",
      "\t2:[f38<0.5] yes=5,no=6,missing=5\n",
      "\t\t5:leaf=0.279891402\n",
      "\t\t6:leaf=-0.488417298\n",
      "\n",
      "[0.5, 0.5, 0.5, 4.784086, 0.20109688, -0.3952522, 4.133449]\n",
      "[0.5, 0.5, -6.039417, 0.1311803, 4.3216853]\n",
      "[0.5, 0.5, 0.5, 0.43507445, -1.073916, 0.2798914, -0.4884173]\n"
     ]
    }
   ],
   "source": [
    "xgb_model0 = xgb.XGBClassifier()\n",
    "xgb_model0.load_model(\"xgb_model.json\")\n",
    "\n",
    "trees = xgb_model0.get_booster().get_dump()\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    \n",
    "with open(\"xgb_model.json\") as json_file:\n",
    "    model_json = json.load(json_file)\n",
    "model = model_json['learner']['gradient_booster']['model']\n",
    "\n",
    "for tree in model['trees']:\n",
    "    print(tree['split_conditions']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453b1a0-bbbc-4300-b867-013182252979",
   "metadata": {},
   "source": [
    "### Alignment\n",
    "\n",
    "Score unbiased data  with the oversampled data trained model ```xgb_model0 ``` to have $p_{train}$, and\n",
    "$$\n",
    "\\text{LO} = \\ln \\frac{p}{1-p}.\n",
    "$$\n",
    "\n",
    "Fit logistic regression model unbiased data as\n",
    "$$\n",
    "\\text{Prob}[Y=1] = \\frac{\\exp(a\\text{LO}+b)}{1+\\exp{(a\\text{LO}+b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f678ae20-48ec-4731-bbd8-79baa3361ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "avg(p)=0.5017, avg(aligned p)=0.4821, avg(y)=0.4821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import logit,expit\n",
    "\n",
    "# prepare log-odds as feature and tru y as label.\n",
    "p_train = xgb_model0.predict_proba(train.iloc[:,:-2])[:,1]\n",
    "lr_df = pd.DataFrame({'LO':pd.Series(p_train).map(logit)})\n",
    "lr_df['y'] = train.iloc[:,-2]\n",
    "\n",
    "#fit logistic regresion to find y = expit(a*LO+b)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(lr_df[['LO']],lr_df['y'])\n",
    "\n",
    "#check avg(p)=avg(y)\n",
    "p_train_aligned = lr.predict_proba(lr_df[['LO']])[:,1]\n",
    "print('Training data:')\n",
    "print(f'avg(p)={p_train.mean():.4f}, avg(aligned p)={p_train_aligned.mean():.4f}, avg(y)={lr_df[\"y\"].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68a5f1-3cd3-48b0-8d5d-f3d9adaad7c4",
   "metadata": {},
   "source": [
    "### Modify model json file\n",
    "\n",
    "Modify\n",
    "```python\n",
    "base_score = model_json['learner']['learner_model_param']['base_score']\n",
    "```\n",
    "to\n",
    "```python\n",
    "expit(a*logit(base_score)+b).\n",
    "```\n",
    "\n",
    "For each ```tree``` in\n",
    "```python\n",
    "model_json['learner']['gradient_booster']['model']['trees']\n",
    "```\n",
    "modify\n",
    "```python\n",
    "tree['split_conditions']\n",
    "```\n",
    "to scale leaf outputs by $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cac33d6-beab-492a-af98-e28364e47ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: avg(adjusted p)=0.4821\n"
     ]
    }
   ],
   "source": [
    "a,b = lr.coef_[0][0], lr.intercept_[0]\n",
    "\n",
    "with open('xgb_model.json') as json_file:\n",
    "    model_json = json.load(json_file)\n",
    "    \n",
    "learner_model_param = model_json['learner']['learner_model_param']\n",
    "base_score = float(learner_model_param['base_score'])\n",
    "base_score_adj = expit(logit(base_score)*a+b)\n",
    "learner_model_param['base_score'] =  str(round(base_score_adj,8))\n",
    "\n",
    "model = model_json['learner']['gradient_booster']['model']\n",
    "for tree in model['trees']:\n",
    "    sc = tree['split_conditions']\n",
    "    for i in range(len(sc)//2,len(sc)):\n",
    "        sc[i]*=a\n",
    "    tree['split_conditions'] = sc\n",
    "    \n",
    "with open(\"xgb_model2.json\",\"w\") as outfile:\n",
    "    json.dump(model_json,outfile)\n",
    "    \n",
    "xgb_model2 = xgb.XGBClassifier()\n",
    "xgb_model2.load_model(\"xgb_model2.json\")\n",
    "\n",
    "p2 = xgb_model2.predict_proba(train.iloc[:,:-2])[:,1]\n",
    "print(f'Training data: avg(adjusted p)={p2.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efe1fb-8963-4637-a5dd-1047382b49c3",
   "metadata": {},
   "source": [
    "## Python class for oversampling adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0e38b5-e24b-4e1e-974a-9193d438fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg(p)    =0.5017\n",
      "avg(p_adj)=0.4821\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class OverSamplingAdjust:\n",
    "    def __init__(self,model_json=None,beta=1):\n",
    "        self.model_json= model_json\n",
    "        self.xgb_model = xgb.XGBClassifier()\n",
    "        self.xgb_model.load_model(self.model_json)\n",
    "        self.beta=beta\n",
    "        self.a=1.0\n",
    "        self.b=0.0\n",
    "\n",
    "    def p_adjust(self,p):\n",
    "        return expit(self.a * logit(p) + self.b)\n",
    "    \n",
    "    def _adjust(self,proba):\n",
    "        p_adj=pd.Series(proba[:,1]).apply(lambda x: x/(x+(1-x)*beta))\n",
    "        return pd.DataFrame({0:1-p_adj,1:p_adj}).values\n",
    "    \n",
    "    def predict(self,X):\n",
    "        proba=self.xgb_model.predict_proba(X)\n",
    "        if self.beta != 1:\n",
    "            proba=self._adjust(proba)\n",
    "        return proba\n",
    "\n",
    "    \n",
    "    def _align(self,X,y):\n",
    "    \n",
    "        #prepare feature and label\n",
    "        self.xgb_model.load_model(self.model_json)\n",
    "        p = pd.Series(self.xgb_model.predict_proba(X)[:,1])\n",
    "        LO=pd.DataFrame({'LO':p.apply(logit)})\n",
    "\n",
    "        # fit logistic regression\n",
    "        lr=LogisticRegression()\n",
    "        lr.fit(LO,y)\n",
    "        self.a,self.b=lr.coef_[0][0],lr.intercept_[0]\n",
    "        \n",
    "    def _update_json(self,model_json_updated='xgb_model_adjusted.json'):\n",
    "        \n",
    "        #load original model json file\n",
    "        with open(self.model_json) as json_file:\n",
    "            model_json = json.load(json_file)\n",
    "        \n",
    "        #update base_score\n",
    "        param = model_json['learner']['learner_model_param']\n",
    "        base_score = float(param['base_score'])\n",
    "        base_score_adj = self.p_adjust(base_score)\n",
    "        param['base_score'] = str(round(base_score_adj,8))\n",
    "        \n",
    "        #update leaf outputs\n",
    "        model = model_json['learner']['gradient_booster']['model']\n",
    "        for tree in model['trees']:\n",
    "            sc=tree['split_conditions']\n",
    "            for i in range(len(sc)//2,len(sc)):\n",
    "                sc[i]*=self.a\n",
    "            tree['split_conditions']=sc\n",
    "\n",
    "        #save updated json to file\n",
    "        with open(model_json_updated, \"w\") as outfile:\n",
    "            json.dump(model_json, outfile)\n",
    "    \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        assert self.model_json is not None, 'model_json file is None'        \n",
    "        self._align(X,y)\n",
    "        self._update_json()\n",
    "        \n",
    "        # update xgb_model by re-loading updated model_json\n",
    "        self.xgb_model = xgb.XGBClassifier()   #MUST initialize xgb again\n",
    "        self.xgb_model.load_model(\"xgb_model_adjusted.json\")\n",
    "        \n",
    "        \n",
    "X=train.iloc[:,:-2]\n",
    "y=train.iloc[:,-2]\n",
    "osa=OverSamplingAdjust(\"xgb_model.json\")    \n",
    "print(f'avg(p)    ={osa.predict(X)[:,1].mean():.4f}')\n",
    "osa.fit(X,y)\n",
    "print(f'avg(p_adj)={osa.predict(X)[:,1].mean():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
