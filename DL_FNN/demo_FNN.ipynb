{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21977b5c-2afd-4b80-b6f2-3b70660aad81",
   "metadata": {},
   "source": [
    "# Demo FNN Training in PyTorch\n",
    "This notebook shows a quick demo of training a feedforward neural network with sklearn breast cancer data.\n",
    "The main steps are \n",
    "- 1.Data preparation: load raw data and create an torch DataLoader object for loading data in training step.\n",
    "- 2.Defining FNN model: define dimensions and activation functions of hidden layers.\n",
    "- 3.Train FNN model: define objective fucntion, optimizer and training loop. In each training epoch, update FNN coefficients with the gradient of the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3600bc7-a04f-4c58-892d-e9797ca90c9e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdea8e0-ee58-4e46-a2a1-01d8b7a76d32",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54486f87-408f-4870-aa22-7d9ca7afd679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02657917 -2.08243377  1.19736196]\n",
      " [ 1.74850201 -0.28733171  1.60760452]\n",
      " [ 1.50226476  0.55799375  1.4898121 ]\n",
      " [-0.81180573  0.34666238 -0.6393874 ]] [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(seed=5)\n",
    "data = load_breast_cancer()\n",
    "x = data.data[:400, 0:3]\n",
    "y = data.target[:400]\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)\n",
    "print(x[:4],y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11887f-e333-4c29-982d-d584fb1ff950",
   "metadata": {},
   "source": [
    "### Create dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044f8711-285d-4370-b58b-4391f1135a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.0266, -2.0824,  1.1974],\n",
      "        [ 1.7485, -0.2873,  1.6076],\n",
      "        [ 1.5023,  0.5580,  1.4898],\n",
      "        [-0.8118,  0.3467, -0.6394]]), tensor([0., 0., 0., 0.]))\n",
      "[tensor([[ 1.0266, -2.0824,  1.1974],\n",
      "        [ 1.7485, -0.2873,  1.6076],\n",
      "        [ 1.5023,  0.5580,  1.4898],\n",
      "        [-0.8118,  0.3467, -0.6394]]), tensor([0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# define dataset class\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "trainset = dataset(x,y)\n",
    "print(trainset[0:4])\n",
    "# create dataloader\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=False)\n",
    "print(next(iter(trainloader)))\n",
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723d764-8c27-4ddc-83eb-a75a54cd57c7",
   "metadata": {},
   "source": [
    "## Define FNN Model\n",
    "Denote the parameters in each layer as $\\boldsymbol\\theta = [\\boldsymbol\\theta_1, \\boldsymbol\\theta_2, \\boldsymbol\\theta_3]$, for each input $x$, \n",
    "$$\n",
    "z_1 = \\sigma[f_1(\\boldsymbol x;\\boldsymbol\\theta_1)], z_2 = \\sigma[f_2(\\boldsymbol z_1;\\boldsymbol\\theta_2)], p = \\sigma[f_3(\\boldsymbol z_2;\\boldsymbol\\theta_3)],\n",
    "$$\n",
    "which is impletemented as\n",
    "```python\n",
    "p = model(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b030a4de-933d-4c30-bb29-70614156dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define FNN Model\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, n1=3, n2=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.f1 = nn.Linear(input_shape,n1)\n",
    "        self.f2 = nn.Linear(n1, n2)\n",
    "        self.f3 = nn.Linear(n2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = torch.sigmoid(self.f1(x))\n",
    "        z2 = torch.sigmoid(self.f2(z1))\n",
    "        x = torch.sigmoid(self.f3(z2))\n",
    "        return x\n",
    "\n",
    "model = Net(input_shape=x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace13fa-5aca-408c-b079-d3ab77e36957",
   "metadata": {},
   "source": [
    "## Train FNN Model\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Use binary cross-entropy for binary $y$ and $p\\in(0,1)$\n",
    "$$\n",
    "H(y,p)= -y\\ln p - (1-y) \\ln(1-p).\n",
    "$$\n",
    "Loss function for each record\n",
    "$$\n",
    "J_i(\\boldsymbol\\theta;\\boldsymbol x_i,y_i) = H[y_i,p(\\boldsymbol x_i;\\!\\boldsymbol\\theta)].\n",
    "$$\n",
    "Total loss of all training records\n",
    "$$\n",
    "J(\\boldsymbol\\theta) = \\sum_i{J_i(\\boldsymbol\\theta;\\boldsymbol x_i,y_i)}.\n",
    "$$\n",
    "In each training epoch, update model as\n",
    "$$\n",
    "\\boldsymbol\\theta \\leftarrow \\boldsymbol\\theta - \\lambda \\frac{\\partial}{\\partial \\boldsymbol\\theta}J(\\boldsymbol\\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b98df0-4614-4139-ae70-a9b45baebf35",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba55d48-47d8-431b-8d70-749b743139d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50: loss=0.695, acuracy=0.5675\n",
      "epoch 100: loss=0.694, acuracy=0.5675\n",
      "epoch 150: loss=0.342, acuracy=0.89\n",
      "epoch 200: loss=0.333, acuracy=0.8925\n",
      "epoch 250: loss=0.329, acuracy=0.895\n",
      "epoch 300: loss=0.279, acuracy=0.895\n",
      "epoch 350: loss=0.209, acuracy=0.8975\n",
      "epoch 400: loss=0.257, acuracy=0.8975\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "learning_rate = 0.1\n",
    "epochs = 400\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct =0\n",
    "    total = 0\n",
    "    for j, (x_train, y_train) in enumerate(trainloader): # for each batch\n",
    "\n",
    "        # calculate loss\n",
    "        p = model(x_train)\n",
    "        loss = loss_fn(p, y_train.reshape(-1,1))\n",
    "\n",
    "        # backpropagation for gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss accumulation for all batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy for the all batches\n",
    "        predicted = (p.detach().numpy() > 0.5).astype(int).reshape(-1)\n",
    "        correct += (predicted == y_train.numpy()).sum()\n",
    "        total += y_train.size(0)\n",
    "    \n",
    "    # output loss and accuracy for every 50 epochs\n",
    "    if (i+1)%50 ==0:\n",
    "        avg_loss = total_loss / len(trainloader)\n",
    "        accuracy = correct /total\n",
    "        print(f\"epoch {i+1}: loss={round(loss.item(),3)}, acuracy={accuracy}\")\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7c103-7a03-4100-9653-561c53909c19",
   "metadata": {},
   "source": [
    "### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd58b02e-ec41-45ef-a00e-5082b86ec67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 96.03, 'gini': 92.06, 'ks': 80.785}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from utils.ROCanalysis import calculate_roc_metrics\n",
    "\n",
    "model_loaded = Net(input_shape=x.shape[1])\n",
    "model_loaded.load_state_dict(torch.load(\"model_weights.pth\", weights_only=True))\n",
    "with torch.no_grad():\n",
    "    all_data = []\n",
    "    all_targets = []\n",
    "    for data, target in trainloader:\n",
    "        all_data.append(data)\n",
    "        all_targets.append(target)\n",
    "    all_data = torch.cat(all_data, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_predictions = model_loaded(all_data)\n",
    "    predictions = all_predictions.detach().numpy()\n",
    "    targets = all_targets.detach().numpy()\n",
    "\n",
    "roc_df = pd.DataFrame({\n",
    "    'p': predictions.flatten(),\n",
    "    'y': targets,\n",
    "    'w': np.ones(len(targets))\n",
    "})\n",
    "roc_metrics = calculate_roc_metrics(roc_df, 'p', 'y')\n",
    "print(roc_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
