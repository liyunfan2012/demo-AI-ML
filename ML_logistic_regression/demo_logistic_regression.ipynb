{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bb311c-8e68-423c-ba0f-53399d90b37f",
   "metadata": {},
   "source": [
    "# Demo Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc05998-d38f-455a-a798-81a7dbd66da4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f897dcd5-453b-4493-8629-cae8cf9ce2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from utils.ROCanalysis import calculate_roc_metrics\n",
    "\n",
    "data = load_breast_cancer()\n",
    "n_features = 4\n",
    "X = data.data[:,:n_features]          # 4 numeric features\n",
    "feature_names = data.feature_names[:n_features].tolist()\n",
    "y = data.target        # 0 = malignant, 1 = benign\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba8a72-6a88-4f8b-a13c-d3b5e391b2e8",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### Model training with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e10026-32a4-4e89-b014-a9ffbc34b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9298245614035088\n",
      "{'auc': 98.347, 'gini': 96.693, 'ks': 85.913}\n",
      "          feature  coefficient\n",
      "0     mean radius    29.469177\n",
      "1    mean texture    -0.986754\n",
      "2  mean perimeter   -26.738366\n",
      "3       mean area    -8.509443\n",
      "4       intercept    -0.222692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(penalty=None,max_iter=10000)  # increase max_iter to ensure convergence\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "accuracy = (y_test == y_pred).sum()/len(y_test)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "\n",
    "proba = log_reg.predict_proba(X_test_scaled)[:,1]\n",
    "roc_df = pd.DataFrame({\n",
    "    'p': proba,\n",
    "    'y': y_test,\n",
    "    'w': np.ones(len(y_test))\n",
    "})\n",
    "print(calculate_roc_metrics(roc_df, 'p', 'y'))\n",
    "\n",
    "all_params = np.concatenate([log_reg.coef_.ravel(), log_reg.intercept_])\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": list(feature_names)+['intercept'],\n",
    "    \"coefficient\": all_params})\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804387d7-bf83-4259-a844-43259d731f8b",
   "metadata": {},
   "source": [
    "### Model training with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d634276-b939-413e-abe0-5290d7e1620b",
   "metadata": {},
   "source": [
    "Features of all samples $X\\in \\mathbb{R}^{n\\times (K+1)}$ including the all one column, labels $\\mathbf{y}\\in {\\mathbb{R}^n}$. The model output of all samples is\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = f(X;\\mathbf{\\beta}).\n",
    "$$\n",
    "\n",
    "The loss is\n",
    "\n",
    "$$\n",
    "l(\\mathbf{\\beta}) = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i\\ln p_i + (1-y_i)\\ln(1-p_i)\\right].\n",
    "$$\n",
    "\n",
    "whose gradient is \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{\\beta}}l(\\mathbf{\\beta}) = \\frac{1}{n} X^T(\\mathbf{p}-\\mathbf{y}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c0261-9c6f-423e-a3b0-7bbdcbd7c361",
   "metadata": {},
   "source": [
    "In python, `beta` is a (K+1,) np.array, `X` is (n,k) np.array.  Adding a all-one column `X` to have `X_new`, (n,k+1) np.array as\n",
    "\n",
    "`X_new = np.hstack([X, np.ones((X.shape[0], 1))])`\n",
    "\n",
    "The probabity of all sample `p` (n,) np.array is calculated as\n",
    "\n",
    "`p = expit((X_new*self.beta).sum(axis=-1))`\n",
    "\n",
    "loss is calculated as\n",
    "\n",
    "`\n",
    "loss = -(y * np.log(p) + (1 - y) * np.log(1-p)).mean()\n",
    "`\n",
    "\n",
    "The gradient is calculated as\n",
    "\n",
    "`\n",
    "grad = X_new.T @ (p - y)/n\n",
    "`\n",
    "\n",
    "Model is trained as\n",
    "\n",
    "`\n",
    "logreg.beta -= lr * (X_new.T @ (p - y)/n)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a1e6c6-02b7-4195-ad75-3d2da52a4f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.19940263619168946\n",
      "[ 10.50456862  -0.93770551 -16.74856317   2.19618139   0.78184272]\n",
      "loss = 0.19316770270187206\n",
      "[ 16.62015457  -0.95284609 -20.84730212  -0.25071182   0.5395708 ]\n",
      "loss = 0.19070757760042098\n",
      "[ 20.51474481  -0.95873387 -22.76249723  -2.57044082   0.322716  ]\n",
      "loss = 0.18955236136765827\n",
      "[ 23.16369384  -0.96396168 -23.93887765  -4.30242174   0.16288009]\n",
      "loss = 0.1889879085791541\n",
      "[ 25.01007901  -0.96906387 -24.75170823  -5.528848     0.05013951]\n",
      "loss = 0.18870760975528852\n",
      "[ 26.3108033   -0.97360294 -25.33553041  -6.38688082  -0.02865227]\n",
      "loss = 0.18856705241345517\n",
      "[ 27.23225113  -0.97734585 -25.75865795  -6.98753106  -0.08380213]\n",
      "loss = 0.1884960934569378\n",
      "[ 27.88725537  -0.98028518 -26.06528264  -7.40973587  -0.12257355]\n",
      "loss = 0.18846009818172804\n",
      "[ 28.35393681  -0.98252351 -26.28698322  -7.70783135  -0.14995336]\n",
      "loss = 0.1884417757171696\n",
      "[ 28.68698228  -0.98419474 -26.44691479  -7.91910061  -0.1693617 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "class LogisticReg:\n",
    "    def __init__(self, dim, eps=0.00000001, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.beta = np.random.rand(dim+1)\n",
    "        self.eps = eps\n",
    "\n",
    "    def _with_intercept(self, X):\n",
    "        return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "    def __call__(self, X):\n",
    "        X_new = self._with_intercept(X)\n",
    "        p = expit(X_new@self.beta)\n",
    "        return p\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        eps = self.eps\n",
    "        p = np.clip(self(X), eps, 1-eps)\n",
    "        loss = -(y * np.log(p) + (1 - y) * np.log(1-p)).mean()\n",
    "        return loss\n",
    "\n",
    "    def loss_grad(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        p = self(X)\n",
    "        X_new = self._with_intercept(X)\n",
    "        grad = X_new.T @ (p - y)/n # no clipping needed for backprop\n",
    "        return grad\n",
    "\n",
    "logreg = LogisticReg(X_train.shape[1])\n",
    "lr = 0.2\n",
    "for i in range(500000):\n",
    "    logreg.beta -= lr*logreg.loss_grad(X_train_scaled,y_train)\n",
    "    if (i+1)%50000==0:\n",
    "        print(f'loss = {logreg.loss(X_train_scaled,y_train)}')\n",
    "        print(logreg.beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
